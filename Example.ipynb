{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83117aac-3deb-4918-a503-4ad1a6d2dfc0",
   "metadata": {},
   "source": [
    "This notebook is inspired by Jerry Ling's documentation:\n",
    "https://moelf.github.io/WVZAnalysis.jl/dev/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abf42e34-b721-484d-9673-eef54f9f17ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m    CondaPkg \u001b[22m\u001b[39m\u001b[0mFound dependencies: /home/grabanal/WVZAnalysis.jl/CondaPkg.toml\n",
      "\u001b[32m\u001b[1m    CondaPkg \u001b[22m\u001b[39m\u001b[0mFound dependencies: /home/grabanal/.julia/packages/PythonCall/eU0yr/CondaPkg.toml\n",
      "\u001b[32m\u001b[1m    CondaPkg \u001b[22m\u001b[39m\u001b[0mDependencies already up to date\n"
     ]
    }
   ],
   "source": [
    "using WVZAnalysis, UnROOT, FHist, Printf, Arrow, Serialization, UnROOT\n",
    "\n",
    "# Pkg.activate(pathof(WVZAnalysis) |> dirname |> dirname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2064ea-8fbc-4c91-8f34-51b90c60ab83",
   "metadata": {},
   "source": [
    "These are all the constant directories presently stored, and they can be changed by the user:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "551f1c39-2133-4851-91bd-a8c2455bf80c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WVZAnalysis.MINITREE_DIR[] = \"/data/jiling/WVZ/v2.3\"\n",
      "WVZAnalysis.ANALYSIS_DIR[] = \"/data/jiling/WVZ/v2.3_hists\"\n",
      "WVZAnalysis.ONNX_MODEL_PATH[] = \"/data/grabanal/NN/NN_08_23.onnx\"\n",
      "WVZAnalysis.BDT_MODEL_PATH[] = \"/data/jiling/WVZ/v2.3-beta2_arrow/xgb_2022-10-17_jerry.model\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"/data/jiling/WVZ/v2.3-beta2_arrow/xgb_2022-10-17_jerry.model\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@show WVZAnalysis.MINITREE_DIR[] # this is where the code will look for input root files\n",
    "@show WVZAnalysis.ANALYSIS_DIR[] # this is where the code will store results\n",
    "@show WVZAnalysis.ONNX_MODEL_PATH[] # this is where the code will load a NN model\n",
    "@show WVZAnalysis.BDT_MODEL_PATH[]  # this is where the code will load a BDT model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c41baf-4ced-4a65-8e8b-7a250fcf6312",
   "metadata": {},
   "source": [
    "The full analysis can create either histograms or dictionaries. \n",
    "- The \"histogram mode\" makes a triplet histograms of NN/BDT score, one for each signal region (SF-inZ, SF-noZ, DF). This  can later be stored as `.root` files and used by TRExFitter.\n",
    "- The \"dictionary mode\" makes a dictionary of physical observables (pt, eta, phi, etc.) as well as weight *for all entries* that pass the analysis cuts, entry by entry. Those dictionaries can be later stored as versatile `.arrow` files and can be imported as dataframes for further studies in Julia, Python, etc.\n",
    "\n",
    "You will set the \"histogram mode\" by `NN_hist = true` and `arrow_making = false`. \n",
    "\n",
    "You will set the \"dictionary mode\" by `NN_hist = false` and `arrow_making = true`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe190dd8-eda0-4481-a540-87fd48370611",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. Running the analysis only on nominal \"Signal\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f738fc9-8ccd-4092-916d-ad3962282ce8",
   "metadata": {},
   "source": [
    "To get the yields only for VVZ, we can use the `\"Signal\"` tag that is defined in `config/file_list.json`. All the tags that can be shown in `WVZAnalysis.ALL_TAGS`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "134f40a0-da93-4b94-8d53-531aefffbb19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"Signal\", \"ZZ\", \"Zjets\", \"Zgamma\", \"ttbar\", \"WZ\", \"tZ\", \"ttZ\", \"tWZ\", \"VBS\", \"VH\", \"Others\")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WVZAnalysis.ALL_TAGS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd02b3a-3148-4813-b732-f8dac22120bd",
   "metadata": {},
   "source": [
    "In order to run the analysis on `\"Signal\"`, we have to: \n",
    "- create one task for each `\"Signal\"` sub-file using the function `prep_tasks`\n",
    "- use the function `main_looper` (the analysis) on those tasks, making a vector of results\n",
    "- add/merge the results\n",
    "\n",
    "To create a task, the function `prep_tasks` needs to know whether the analysis will create histograms or dictionaries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080c4786-50db-46f2-b101-983d9c5c0b4e",
   "metadata": {},
   "source": [
    "#### a) Using the histogram mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84634be2-9f93-4038-988f-c4b69b0ba29a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signal yield: \n",
      "- in SF-inZ:  11.73 ± 0.07\n",
      "- in SF-noZ:  9.31 ± 0.10\n",
      "- in DF:      10.73 ± 0.14\n",
      "Total:        31.77 ± 0.19\n"
     ]
    }
   ],
   "source": [
    "signal_tasks = prep_tasks(\"Signal\"; arrow_making=false, NN_hist=true, shape_variation=\"NOMINAL\"); \n",
    "vector = map(main_looper, signal_tasks)\n",
    "res = reduce(mergewith(+), deepcopy(vector))\n",
    "\n",
    "N     = nbins(res[:DF__NN__NOMINAL])\n",
    "hists = rebin.([res[:SFinZ__NN__NOMINAL], res[:SFnoZ__NN__NOMINAL], res[:DF__NN__NOMINAL]], N);\n",
    "\n",
    "@printf(\"Signal yield: \\n\")\n",
    "@printf(\"- in SF-inZ:  %0.2f ± %0.2f\\n\", integral(hists[1]), only(binerrors(hists[1])))\n",
    "@printf(\"- in SF-noZ:  %0.2f ± %0.2f\\n\", integral(hists[2]), only(binerrors(hists[2])))\n",
    "@printf(\"- in DF:      %0.2f ± %0.2f\\n\", integral(hists[3]), only(binerrors(hists[3])))\n",
    "@printf(\"Total:        %0.2f ± %0.2f\\n\", sum(map(integral,hists)), sqrt(sum(map(only∘binerrors, hists).^2)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d259f2b3-fc8a-4ff2-bd62-bef941b014ea",
   "metadata": {},
   "source": [
    "You can save the result as `.jlserialize` or `.root`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f40bc2-c9d0-4db2-a843-56fbd8e82419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name for the root file\n",
    "name = \"test_hist\" \n",
    "# Target folder\n",
    "p = WVZAnalysis.ANALYSIS_DIR[] \n",
    "\n",
    "#################################\n",
    "### Saving it as .jlserialize ###\n",
    "#################################\n",
    "# using Serialization\n",
    "# serialize(joinpath(p,\"$(name).jlserialize\"), res)\n",
    "\n",
    "##########################\n",
    "### Saving it as .root ###\n",
    "##########################\n",
    "# using PythonCall\n",
    "\n",
    "# function make_TH1D(h)\n",
    "#     np = pyimport(\"numpy\")\n",
    "#     pyhist = pyimport(\"hist\")\n",
    "\n",
    "#     hout = pyhist.Hist(pyhist.axis.Regular(100, 0, 1), storage=pyhist.storage.Weight())\n",
    "#     bc = bincounts(h)\n",
    "#     va = h.sumw2\n",
    "#     hout[pybuiltins.Ellipsis] = np.stack([bc, va], axis=-1)\n",
    "#     return hout\n",
    "# end\n",
    "\n",
    "# up = pyimport(\"uproot\")\n",
    "# pywith(up.recreate(joinpath(p, \"$(name).root\"))) do file\n",
    "#     for (k,v) in res\n",
    "#         file[string(k)] = make_TH1D(v)\n",
    "#     end\n",
    "# end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273ca932-c567-4960-97dd-714b9aceee1f",
   "metadata": {},
   "source": [
    "The analysis package actually has functions that can do these things for all processes and all systematics in just a few of lines. It just takes several more lines in this example because I am focusing on the details involved in processing a single process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af2b987-72a9-4fc4-b81f-48830fc2c948",
   "metadata": {},
   "source": [
    "#### b) Using the dictionary mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f8ec01-c69f-4875-a391-f625918c818f",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_tasks = prep_tasks(\"Signal\"; arrow_making=true, NN_hist=false, shape_variation=\"NOMINAL\"); \n",
    "vector = map(main_looper, signal_tasks)\n",
    "res = reduce(mergewith(append!), deepcopy(vector))\n",
    "\n",
    "@printf(\"Signal yield: \\n\")\n",
    "@printf(\"- in SF-inZ:  %0.2f ± %0.2f\\n\", sum(res[:wgt][res[:SR] .== 0]), sqrt(sum(res[:wgt][res[:SR] .== 0].^2)))\n",
    "@printf(\"- in SF-noZ:  %0.2f ± %0.2f\\n\", sum(res[:wgt][res[:SR] .== 1]), sqrt(sum(res[:wgt][res[:SR] .== 1].^2)))\n",
    "@printf(\"- in DF:      %0.2f ± %0.2f\\n\", sum(res[:wgt][res[:SR] .== 2]), sqrt(sum(res[:wgt][res[:SR] .== 2].^2)))\n",
    "@printf(\"Total:        %0.2f ± %0.2f\\n\", sum(res[:wgt]), sqrt(sum(res[:wgt].^2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551bd7be-d97d-40d2-9925-3a185ae72e41",
   "metadata": {},
   "source": [
    "You can save the result as `.jlserialize` or `.arrow`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ee031fa-abfb-4985-be2b-b66f35280d29",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: WVZAnalysis not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: WVZAnalysis not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[1]:4",
      " [2] eval",
      "   @ ./boot.jl:368 [inlined]",
      " [3] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base ./loading.jl:1428"
     ]
    }
   ],
   "source": [
    "# Name for the root file\n",
    "name = \"test_dict\" \n",
    "# Target folder\n",
    "p = WVZAnalysis.ANALYSIS_DIR[] \n",
    "\n",
    "#################################\n",
    "### Saving it as .jlserialize ###\n",
    "#################################\n",
    "# using Serialization\n",
    "# serialize(joinpath(p,\"$(name).jlserialize\"), res)\n",
    "\n",
    "###########################\n",
    "### Saving it as .arrow ###\n",
    "###########################\n",
    "# using Arrow\n",
    "# Arrow.write(joinpath(p,\"$(name).arrow\"), res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5e826c-bf53-464a-bd94-b87eefc11112",
   "metadata": {},
   "source": [
    "The analysis package actually has functions that can do these things for all processes and all systematics in just a few of lines. It just takes several more lines in this example because I am focusing on the details involved in processing a single process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cffae9d-90f6-4640-9183-b3746ce5b2a0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2. Significance table of all processes in two lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39059503-428f-40c2-9106-b7105e6e8ce0",
   "metadata": {},
   "source": [
    "All posible tags can be found in `config/file_list.json`, it is there where we define the paths for the root files of \"Signal\", \"ZZ\", \"WZ\", etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a474a5f1-cdf0-43a8-8fe2-552475d4546b",
   "metadata": {},
   "source": [
    "The following line allows you to analyze at once all the physics processes (signal and backgrounds) in each signal region, as well as their significance in quadrature and print it out as a nice yield table. Using a single thread, this line takes just about 30 minutes with `recreate=true`. The way this works is that internally, the function `significance_table` calls upon the `prep_tasks` and `main_looper` as described in the section above, except it does it for all processes.\n",
    "\n",
    "This will also store the results of the analysis as `.jlserialize` files in the directory `WVZAnalysis.ANALYSIS_DIR[]` so that for the next runs of the code, we can just read those files using `recreate=false` and this will take only ~15 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94ea19c2-0425-4b1a-8938-ecb837c4ad99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"/data/grabanal/WVZ/v2.3_hists\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WVZAnalysis.ANALYSIS_DIR[] = \"/data/grabanal/WVZ/v2.3_hists\"\n",
    "WVZAnalysis.ANALYSIS_DIR[]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6893239-4284-4bb5-8ef4-2585aeae6202",
   "metadata": {},
   "source": [
    "The following cell will not work if it's the first time you run the analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a791373a-8166-480f-9c28-18508118e3e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌───────────────┬────────────────┬───────────────┬──────────────┐\n",
      "│\u001b[1m               \u001b[0m│\u001b[1m     SF-inZ     \u001b[0m│\u001b[1m    SF-noZ     \u001b[0m│\u001b[1m      DF      \u001b[0m│\n",
      "├───────────────┼────────────────┼───────────────┼──────────────┤\n",
      "│\u001b[1m    Signal     \u001b[0m│\u001b[1m  11.73 ± 0.07  \u001b[0m│\u001b[1m  9.31 ± 0.1   \u001b[0m│\u001b[1m 10.73 ± 0.14 \u001b[0m│\n",
      "│      ZZ       │ 1775.6 ± 5.89  │ 469.06 ± 2.44 │ 19.78 ± 0.45 │\n",
      "│     Zjets     │  -0.02 ± 0.13  │  2.6 ± 2.22   │ 6.47 ± 5.51  │\n",
      "│    Zgamma     │   0.0 ± 0.0    │   0.0 ± 0.0   │  0.3 ± 0.29  │\n",
      "│     ttbar     │   0.0 ± 0.0    │  0.63 ± 0.18  │  0.28 ± 0.1  │\n",
      "│      WZ       │   0.36 ± 0.1   │  1.79 ± 0.23  │ 2.24 ± 0.29  │\n",
      "│      tZ       │  0.01 ± 0.01   │  0.07 ± 0.03  │ 0.06 ± 0.02  │\n",
      "│      ttZ      │  1.24 ± 0.08   │  4.71 ± 0.16  │ 5.74 ± 0.18  │\n",
      "│      tWZ      │  0.57 ± 0.11   │  2.16 ± 0.23  │  2.5 ± 0.24  │\n",
      "│      VBS      │  11.71 ± 0.09  │  6.4 ± 0.08   │ 0.18 ± 0.01  │\n",
      "│      VH       │  1.29 ± 0.71   │  5.76 ± 1.4   │ 5.77 ± 1.29  │\n",
      "│    Others     │  0.06 ± 0.01   │  0.4 ± 0.13   │ 0.56 ± 0.08  │\n",
      "├───────────────┼────────────────┼───────────────┼──────────────┤\n",
      "│   Bkg Tot.    │ 1790.82 ± 5.94 │ 493.58 ± 3.61 │ 43.89 ± 5.7  │\n",
      "├───────────────┼────────────────┼───────────────┼──────────────┤\n",
      "│ Significance  │   0.28 ± 0.0   │  0.42 ± 0.0   │  1.56 ± 0.1  │\n",
      "│ Combined Sig. │   NaN ± 0.0    │  1.64 ± 0.09  │  NaN ± 0.0   │\n",
      "└───────────────┴────────────────┴───────────────┴──────────────┘\n",
      " 25.481712 seconds (11.85 M allocations: 627.732 MiB, 1.68% gc time, 21.78% compilation time: 2% of which was recompilation)\n"
     ]
    }
   ],
   "source": [
    "# @time begin \n",
    "#     M = significance_table(; recreate=false);\n",
    "#     print_sigtable(M)\n",
    "# end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12c1926-7296-4a63-a9e8-4591421fce43",
   "metadata": {},
   "source": [
    "The following cell will recreate all the `.jlserialize` files for all the nominal processes *and* make the significance table. It can take a while (at least 25 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c521f88-cfe4-4d4e-9b81-8866591463eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @time begin \n",
    "#     M = significance_table(; recreate=true);\n",
    "#     print_sigtable(M)\n",
    "# end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1487b565-96c7-49ed-8728-871ad7aae24f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 3. Analyzing a single DSID / MC Campaign"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378d4cf9-4bcb-467c-9a05-473f2e9d49f9",
   "metadata": {},
   "source": [
    "For this example, I will analyze a ZZ file with DSID 364250 for the mc16a campaign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a17e5e9c-8a93-4a99-ba81-30c78445f39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = \"/data/rjacobse/WVZ/v2.3/user.jiling.WVZ_v2.3sf.364250.e5894_s3126_r9364_p4434_ANALYSIS.root/\";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e506bf6-821d-4d21-bd02-2c8a6a3cf77d",
   "metadata": {},
   "source": [
    "Looking at the filename ending, it looks like a `.root` file. However, it is actually a folder that contains several `.root` files because this is how the minitrees are produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "adfb39f6-14b6-4e02-ad25-c60268a47701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Vector{String}:\n",
       " \"sumsumWeight.txt\"\n",
       " \"user.jiling.29896114._000001.ANALYSIS.root\"\n",
       " \"user.jiling.29896114._000002.ANALYSIS.root\"\n",
       " \"user.jiling.29896114._000003.ANALYSIS.root\""
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "readdir(dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2aa90fc-7208-4eff-9def-92f8b7f9f0a3",
   "metadata": {},
   "source": [
    "Point in fact, there are *three* `.root` files inside this folder (there is also a `.txt` file which can be ignored for now)\n",
    "\n",
    "Each one of them has a quantity `sumWeight`. The entries in all these three files belong to the same DSID and MC campaign so this is why we need to sum all the `sumWeight` of each `.root` file to get the \"real\" sum of weights. It is this quantity, that we call `sumsumWeight` that the MC weights will have to be divided by. This is how it looks like for these files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cc5fe459-00ca-48d4-bcb9-43f4dbd11ea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.518811876953125e6"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sumsumWeight = 0.0\n",
    "for f in readdir(dir_path; join=true)\n",
    "    if endswith(f, \".root\")\n",
    "        r = ROOTFile(f)\n",
    "        sumsumWeight += r[\"sumWeight\"][:fN][3]\n",
    "    end\n",
    "end\n",
    "\n",
    "sumsumWeight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b3f0d905-83ab-42d0-be9a-bb0876132a3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.518811876953125e6"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Another way\n",
    "subfiles = readdir(dir_path; join=true)\n",
    "subfiles = filter(endswith(\".root\"), subfiles)\n",
    "sumsumWeight = mapreduce(x -> ROOTFile(x)[\"sumWeight\"][:fN][3], +, subfiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db9df81-dc40-4cac-8ec0-0049e0c508c1",
   "metadata": {},
   "source": [
    "Now I will make an `AnalysisTask` object. \n",
    "This is the atomic unit of runnable task, it runs on a single `.root` file and it needs the `sumsumWeights` (and other optional arguments) as input to fully apply the analysis to this file. \n",
    "For this example, the output of the analysis will be a dictionary of physical observables.\n",
    "Since the `AnalysisTask` object can run on a single `.root` file at a time, I can do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7787e990-12dd-4f4c-9d21-d01055aa8caa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{Symbol, Hist1D} with 3 entries:\n",
       "  :SFinZ__NN__NOMINAL => Hist1D{Float64}, edges=0.0:0.01:1.0, integral=87.90420…\n",
       "  :DF__NN__NOMINAL    => Hist1D{Float64}, edges=0.0:0.01:1.0, integral=0.946533…\n",
       "  :SFnoZ__NN__NOMINAL => Hist1D{Float64}, edges=0.0:0.01:1.0, integral=24.30252…"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_path = joinpath(dir_path, \"user.jiling.29896114._000001.ANALYSIS.root\");\n",
    "\n",
    "my_task = AnalysisTask(path            = root_path, \n",
    "                       sumWeight       = sumsumWeight,\n",
    "                       arrow_making    = false, # We are choosing to make histograms this time but feel free to change\n",
    "                       NN_hist         = true,  # We are choosing to make histograms this time but feel free to change\n",
    "                       shape_variation = \"NOMINAL\");\n",
    "\n",
    "res = main_looper(my_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8798949a-99ca-43e6-9c16-8c45b54bc504",
   "metadata": {},
   "source": [
    "Now, `main_looper` is running the full analysis on this `AnalysisTask`.\n",
    "\n",
    "- If `NN_hist = true` and `arrow_making = false`, result will be a triplet of histograms, one for each signal region (SF-inZ, SF-noZ, DF)\n",
    "- If `NN_hist = false` and `arrow_making = true`, result will be a dictionary, where the \"keys\" are different magnitudes that we chose, and the \"values\" are the actual values event by event:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25308d49-9f10-4087-9825-3b9d2390267e",
   "metadata": {},
   "source": [
    "Getting the yield (integral):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "295f3ba7-bf03-49a7-9124-5817a7ed5d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The yield of this subfile: \n",
      "- in SF-inZ:  87.90 ± 1.18\n",
      "- in SF-noZ:  24.30 ± 0.48\n",
      "- in DF:      0.95 ± 0.09\n",
      "Total:        113.15 ± 1.28\n"
     ]
    }
   ],
   "source": [
    "#########################\n",
    "### In histogram mode ###\n",
    "#########################\n",
    "N     = nbins(res[:DF__NN__NOMINAL])\n",
    "hists = rebin.([res[:SFinZ__NN__NOMINAL], res[:SFnoZ__NN__NOMINAL], res[:DF__NN__NOMINAL]], N);\n",
    "@printf(\"The yield of this subfile: \\n\")\n",
    "@printf(\"- in SF-inZ:  %0.2f ± %0.2f\\n\", integral(hists[1]), only(binerrors(hists[1])))\n",
    "@printf(\"- in SF-noZ:  %0.2f ± %0.2f\\n\", integral(hists[2]), only(binerrors(hists[2])))\n",
    "@printf(\"- in DF:      %0.2f ± %0.2f\\n\", integral(hists[3]), only(binerrors(hists[3])))\n",
    "@printf(\"Total:        %0.2f ± %0.2f\\n\", sum(map(integral,hists)), sqrt(sum(map(only∘binerrors, hists).^2)) )\n",
    "\n",
    "##########################\n",
    "### In dictionary mode ###\n",
    "##########################\n",
    "# @printf(\"Signal yield: \\n\")\n",
    "# @printf(\"- in SF-inZ:  %0.2f ± %0.2f\\n\", sum(res[:wgt][res[:SR] .== 0]), sqrt(sum(res[:wgt][res[:SR] .== 0].^2)))\n",
    "# @printf(\"- in SF-noZ:  %0.2f ± %0.2f\\n\", sum(res[:wgt][res[:SR] .== 1]), sqrt(sum(res[:wgt][res[:SR] .== 1].^2)))\n",
    "# @printf(\"- in DF:      %0.2f ± %0.2f\\n\", sum(res[:wgt][res[:SR] .== 2]), sqrt(sum(res[:wgt][res[:SR] .== 2].^2)))\n",
    "# @printf(\"Total:        %0.2f ± %0.2f\\n\", sum(res[:wgt]), sqrt(sum(res[:wgt].^2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36b8a5f-36a1-4586-b9a8-5f42a0bee3cf",
   "metadata": {},
   "source": [
    "This processes one single sub-file.\n",
    "However, if you recall there are actually 3 sub-files that need to be processed. \n",
    "I can just do a for-loop to process all 3. It is a large \"ZZ\" file so it could take at least around 2~3 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "756e73e9-e05a-4761-a6fd-964ca3074a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sumsumWeight is 7.518811876953125e6\n",
      "--- Analyzing /data/rjacobse/WVZ/v2.3/user.jiling.WVZ_v2.3sf.364250.e5894_s3126_r9364_p4434_ANALYSIS.root/user.jiling.29896114._000001.ANALYSIS.root ... \n",
      "--- Pushing into the vector \n",
      "--- Analyzing /data/rjacobse/WVZ/v2.3/user.jiling.WVZ_v2.3sf.364250.e5894_s3126_r9364_p4434_ANALYSIS.root/user.jiling.29896114._000002.ANALYSIS.root ... \n",
      "--- Pushing into the vector \n",
      "--- Analyzing /data/rjacobse/WVZ/v2.3/user.jiling.WVZ_v2.3sf.364250.e5894_s3126_r9364_p4434_ANALYSIS.root/user.jiling.29896114._000003.ANALYSIS.root ... \n",
      "--- Pushing into the vector \n"
     ]
    }
   ],
   "source": [
    "vector = [] # will be a vector\n",
    "\n",
    "# Getting sumsumWeight\n",
    "sumsumWeight = 0.0\n",
    "for f in readdir(dir_path; join=true)\n",
    "    if endswith(f, \".root\")\n",
    "        r = ROOTFile(f)\n",
    "        sumsumWeight += r[\"sumWeight\"][:fN][3]\n",
    "    end\n",
    "end\n",
    "print(\"sumsumWeight is \", sumsumWeight, \"\\n\")\n",
    "\n",
    "# Processing a task for each .root file\n",
    "for f in readdir(dir_path; join=true)\n",
    "    if endswith(f, \".root\")\n",
    "        r = ROOTFile(f)\n",
    "        print(\"--- Analyzing \", f, \" ... \\n\")\n",
    "        myAnalysisTask = AnalysisTask(path = f, \n",
    "                                      sumWeight = sumsumWeight, # using sumsumWeight\n",
    "                                      arrow_making=false, \n",
    "                                      NN_hist=true,\n",
    "                                      shape_variation = \"NOMINAL\")\n",
    "        print(\"--- Pushing into the vector \\n\")\n",
    "        push!(vector, main_looper(myAnalysisTask))\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488a473c-4668-4de6-a70c-553bf28e71da",
   "metadata": {},
   "source": [
    "I can now `merge` all the 3 results with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f98a424f-021e-4de9-9552-baeb13ca65c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{Symbol, Hist1D} with 3 entries:\n",
       "  :SFinZ__NN__NOMINAL => Hist1D{Float64}, edges=0.0:0.01:1.0, integral=444.3999…\n",
       "  :DF__NN__NOMINAL    => Hist1D{Float64}, edges=0.0:0.01:1.0, integral=5.107544…\n",
       "  :SFnoZ__NN__NOMINAL => Hist1D{Float64}, edges=0.0:0.01:1.0, integral=101.1958…"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#########################\n",
    "### In histogram mode ###\n",
    "#########################\n",
    "res = reduce(mergewith(+), deepcopy(vector))\n",
    "\n",
    "##########################\n",
    "### In dictionary mode ###\n",
    "##########################\n",
    "# res = reduce(mergewith(append!), deepcopy(vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5b70b0c-9ca8-41dd-9d11-c1485d40dedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nominal yield for this ZZ file: \n",
      "- in SF-inZ:  444.40 ± 2.25\n",
      "- in SF-noZ:  101.20 ± 1.01\n",
      "- in DF:      5.11 ± 0.26\n",
      "Total:        550.70 ± 2.48\n"
     ]
    }
   ],
   "source": [
    "#########################\n",
    "### In histogram mode ###\n",
    "#########################\n",
    "N     = nbins(res[:DF__NN__NOMINAL])\n",
    "hists = rebin.([res[:SFinZ__NN__NOMINAL], res[:SFnoZ__NN__NOMINAL], res[:DF__NN__NOMINAL]], N);\n",
    "@printf(\"Nominal yield for this ZZ file: \\n\")\n",
    "@printf(\"- in SF-inZ:  %0.2f ± %0.2f\\n\", integral(hists[1]), only(binerrors(hists[1])))\n",
    "@printf(\"- in SF-noZ:  %0.2f ± %0.2f\\n\", integral(hists[2]), only(binerrors(hists[2])))\n",
    "@printf(\"- in DF:      %0.2f ± %0.2f\\n\", integral(hists[3]), only(binerrors(hists[3])))\n",
    "@printf(\"Total:        %0.2f ± %0.2f\\n\", sum(map(integral,hists)), sqrt(sum(map(only∘binerrors, hists).^2)) )\n",
    "\n",
    "##########################\n",
    "### In dictionary mode ###\n",
    "##########################\n",
    "# @printf(\"Nominal yield for this ZZ file: \\n\")\n",
    "# @printf(\"- in SF-inZ:  %0.2f ± %0.2f\\n\", sum(res[:wgt][res[:SR] .== 0]), sqrt(sum(res[:wgt][res[:SR] .== 0].^2)))\n",
    "# @printf(\"- in SF-noZ:  %0.2f ± %0.2f\\n\", sum(res[:wgt][res[:SR] .== 1]), sqrt(sum(res[:wgt][res[:SR] .== 1].^2)))\n",
    "# @printf(\"- in DF:      %0.2f ± %0.2f\\n\", sum(res[:wgt][res[:SR] .== 2]), sqrt(sum(res[:wgt][res[:SR] .== 2].^2)))\n",
    "# @printf(\"Total:        %0.2f ± %0.2f\\n\", sum(res[:wgt]), sqrt(sum(res[:wgt].^2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab9e978-5e07-43ab-9bde-e1cd83898a90",
   "metadata": {},
   "source": [
    "- The rationale behind having an `AnalysisTask` object, is that it can be parallelized. Also this example used the nominal sample, but it can also handle systematic trees. \n",
    "- This example showed how to produce dictionaries. But by changing an argument in `AnalysisTask` we can make it produce directly simple histograms instead. Dictionaries are necessary for machine learning, while for TRExFitter we just need histograms.\n",
    "- This looks a bit convoluted because I am analyzing a single file. But we have several functions in the repository that allow us to run the full analysis on all samples (with as many systematics as you want) with just a few lines of code, as shown in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c60dc5-540b-46eb-b126-db03864987c4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.2",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
